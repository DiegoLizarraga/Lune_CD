# üöÄ Gu√≠a de Instalaci√≥n: Lune con Ollama + LangChain

## Paso 1: Instalar Ollama

### Windows
1. Descarga Ollama desde: https://ollama.com/download/windows
2. Ejecuta el instalador
3. Ollama se iniciar√° autom√°ticamente como servicio

### macOS
```bash
brew install ollama
```

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## Paso 2: Verificar que Ollama est√© corriendo

```bash
# En una terminal, ejecuta:
ollama --version
```

Si ves la versi√≥n, ¬°Ollama est√° instalado correctamente!

## Paso 3: Descargar un modelo

Elige un modelo seg√∫n tu hardware:

### Para PCs con 4-6GB RAM (ligero y r√°pido)
```bash
ollama pull phi3:mini
```

### Para PCs con 8-12GB RAM (recomendado)
```bash
ollama pull llama3.2:3b
```

### Para PCs con 16GB+ RAM (mejor calidad)
```bash
ollama pull llama3.1:8b
```

### Para mejor soporte en espa√±ol
```bash
ollama pull nous-hermes2
```

## Paso 4: Actualizar dependencias de Python

```bash
pip install -r requirements_enhanced.txt
```

## Paso 5: Actualizar main.py

Reemplaza esta l√≠nea en `main.py`:
```python
from local_model import LocalModel
```

Por:
```python
from enhanced_model import EnhancedLocalModel
```

Y cambia la inicializaci√≥n:
```python
# Antes:
self.local_model = LocalModel()

# Despu√©s:
self.local_model = EnhancedLocalModel(
    model_name="llama3.2:3b",  # o el modelo que descargaste
    use_ollama=True
)
```

## üéØ Modelos Recomendados

| Modelo | RAM M√≠nima | Velocidad | Calidad | Espa√±ol |
|--------|-----------|-----------|---------|---------|
| `phi3:mini` | 4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚≠ê‚≠ê |
| `llama3.2:3b` | 6GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| `llama3.1:8b` | 12GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `nous-hermes2` | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `mistral` | 8GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

## üîß Comandos √ötiles de Ollama

```bash
# Ver modelos instalados
ollama list

# Descargar un modelo
ollama pull <modelo>

# Eliminar un modelo
ollama rm <modelo>

# Probar un modelo en terminal
ollama run llama3.2:3b

# Detener Ollama
# Windows: Busca "Ollama" en Servicios y detenerlo
# Linux/Mac: killall ollama
```

## üé® Ventajas del Nuevo Sistema

### Con Ollama + LangChain:
‚úÖ Respuestas mucho m√°s naturales y contextuales
‚úÖ Memoria de conversaci√≥n (recuerda las √∫ltimas 5 interacciones)
‚úÖ Entiende contexto y puede mantener hilos de conversaci√≥n
‚úÖ Mejor comprensi√≥n del lenguaje natural
‚úÖ Respuestas m√°s precisas y relevantes
‚úÖ 100% privado - todo local

### Fallback autom√°tico:
‚úÖ Si Ollama no est√° disponible, usa el modelo de reglas
‚úÖ No se rompe la aplicaci√≥n
‚úÖ Transici√≥n transparente

## üêõ Soluci√≥n de Problemas

### "Ollama no responde"
```bash
# Windows: Reinicia el servicio de Ollama
# Linux/Mac:
ollama serve
```

### "Error al descargar modelo"
```bash
# Verifica tu conexi√≥n a internet y prueba:
ollama pull llama3.2:3b --insecure
```

### "Respuestas muy lentas"
- Usa un modelo m√°s ligero (phi3:mini)
- Cierra otras aplicaciones
- Verifica que tengas suficiente RAM disponible

### "Import Error: langchain"
```bash
pip install --upgrade langchain langchain-community ollama-python
```

## üìä Comparaci√≥n: Antes vs Despu√©s

### ANTES (Modelo basado en reglas)
```
Usuario: "¬øC√≥mo puedo organizar mejor mi d√≠a?"
Lune: "No estoy segura de entender. Puedes decirme 'ayuda'..."
```

### DESPU√âS (Con Ollama)
```
Usuario: "¬øC√≥mo puedo organizar mejor mi d√≠a?"
Lune: "Te recomiendo usar mis funciones de notas y recordatorios. 
Puedes crear una lista de tareas con 'toma nota' y programar 
recordatorios con 'recu√©rdame'. Tambi√©n puedo ayudarte a priorizar 
si me cuentas qu√© tienes pendiente."
```

## üöÄ Pr√≥ximos Pasos

Una vez que todo funcione:

1. **Experimenta con diferentes modelos** para encontrar el mejor para ti
2. **Personaliza el prompt** en `enhanced_model.py` para ajustar la personalidad de Lune
3. **Aumenta la memoria** cambiando `k=5` a `k=10` para conversaciones m√°s largas
4. **Implementa RAG** (Retrieval Augmented Generation) para que Lune recuerde informaci√≥n de sesiones anteriores

## üí° Tips de Uso

- Los modelos m√°s grandes (8b, 13b) dan mejores respuestas pero son m√°s lentos
- Ollama usa GPU si est√° disponible, lo que acelera mucho las respuestas
- Puedes tener m√∫ltiples modelos instalados y cambiar entre ellos
- El primer mensaje siempre es m√°s lento (el modelo se est√° cargando)

## üìö Recursos Adicionales

- [Documentaci√≥n de Ollama](https://github.com/ollama/ollama)
- [Documentaci√≥n de LangChain](https://python.langchain.com/)
- [Lista de modelos disponibles](https://ollama.com/library)
- [Comunidad de Ollama en Discord](https://discord.gg/ollama)

---

¬øProblemas? Abre un issue en el repositorio o contacta al desarrollador.